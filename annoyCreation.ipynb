{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Collecting annoy\n",
      "  Downloading annoy-1.17.3.tar.gz (647 kB)\n",
      "     ---------------------------------------- 0.0/647.5 kB ? eta -:--:--\n",
      "     ---------------- ----------------------- 262.1/647.5 kB ? eta -:--:--\n",
      "     -------------------------------------- 647.5/647.5 kB 1.8 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Building wheels for collected packages: annoy\n",
      "  Building wheel for annoy (pyproject.toml): started\n",
      "  Building wheel for annoy (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for annoy: filename=annoy-1.17.3-cp312-cp312-win_amd64.whl size=52397 sha256=c14f927eaa7041de479fcb8aaa5089e2b55fd90db07b153e350becdaf8389b7d\n",
      "  Stored in directory: c:\\users\\konrad_master\\appdata\\local\\pip\\cache\\wheels\\db\\b9\\53\\a3b2d1fe1743abadddec6aa541294b24fdbc39d7800bc57311\n",
      "Successfully built annoy\n",
      "Installing collected packages: annoy\n",
      "Successfully installed annoy-1.17.3\n"
     ]
    }
   ],
   "source": [
    "#pip install annoy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\konrad_master\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from disk...\n",
      "Dataset loaded successfully!\n",
      "Loading SentenceTransformer model...\n",
      "Model loaded successfully!\n",
      "cuda\n",
      "Initialized Annoy index.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\konrad_master\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253.15101718902588\n",
      "Starting to process dataset in batches...\n",
      "Finished processing all batches!\n",
      "Building Annoy index...\n",
      "Annoy index built successfully!\n",
      "Saving Annoy index...\n",
      "Annoy index saved successfully!\n"
     ]
    }
   ],
   "source": [
    "from annoy import AnnoyIndex\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "import torch\n",
    "\n",
    "# Load the dataset from the relative directory\n",
    "print(\"Loading dataset from disk...\")\n",
    "\n",
    "ds = load_dataset(\"wikimedia/wikipedia\", \"20231101.en\")\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "\n",
    "# Load the SentenceTransformer model\n",
    "print(\"Loading SentenceTransformer model...\")\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "print(device)\n",
    "\n",
    "# Initialize Annoy index\n",
    "f = model.get_sentence_embedding_dimension()\n",
    "t = AnnoyIndex(f, 'euclidean')\n",
    "print(\"Initialized Annoy index.\")\n",
    "\n",
    "start = time.time()\n",
    "embeddings = ds['train']['text'][:256]\n",
    "embeddings = model.encode(embeddings, batch_size=256, device=device)  # Encode the single text\n",
    "# Process dataset in batches\n",
    "end = time.time()\n",
    "print(end-start)\n",
    "print(\"Starting to process dataset in batches...\")\n",
    "for i, embedding in enumerate(embeddings):\n",
    "    t.add_item(i, embedding)\n",
    "\n",
    "\n",
    "print(\"Finished processing all batches!\")\n",
    "\n",
    "# Build and save the Annoy index\n",
    "print(\"Building Annoy index...\")\n",
    "t.build(10)  # Use more trees for better accuracy\n",
    "print(\"Annoy index built successfully!\")\n",
    "\n",
    "print(\"Saving Annoy index...\")\n",
    "t.save('highQualityAnnoyEuclidean.ann')\n",
    "print(\"Annoy index saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from disk...\n",
      "Dataset loaded successfully!\n",
      "Loading SentenceTransformer model...\n",
      "Model loaded successfully!\n",
      "Using device: cuda\n",
      "Dividing dataset into 10 parts, each with approximately 640782 texts.\n",
      "Processing part 1/10 with 640782 texts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches:   7%|â–‹         | 163/2504 [15:24<20:28,  1.91it/s]   "
     ]
    }
   ],
   "source": [
    "from annoy import AnnoyIndex\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "import torch\n",
    "import math\n",
    "import os\n",
    "\n",
    "# Load the dataset from the relative directory\n",
    "print(\"Loading dataset from disk...\")\n",
    "ds = load_dataset(\"wikimedia/wikipedia\", \"20231101.en\")\n",
    "print(\"Dataset loaded successfully!\")\n",
    "\n",
    "# Load the SentenceTransformer model\n",
    "print(\"Loading SentenceTransformer model...\")\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize variables\n",
    "num_parts = 10  # Number of parts to divide the dataset\n",
    "f = model.get_sentence_embedding_dimension()  # Dimension of embeddings\n",
    "output_dir = \"annoy_indexes\"  # Directory to save Annoy files\n",
    "os.makedirs(output_dir, exist_ok=True)  # Ensure the output directory exists\n",
    "\n",
    "# Split the dataset\n",
    "texts = ds['train']['text']\n",
    "total_texts = len(texts)\n",
    "part_size = math.ceil(total_texts / num_parts)\n",
    "\n",
    "print(f\"Dividing dataset into {num_parts} parts, each with approximately {part_size} texts.\")\n",
    "\n",
    "# Process each part\n",
    "for part in range(num_parts):\n",
    "    start_idx = part * part_size\n",
    "    end_idx = min(start_idx + part_size, total_texts)\n",
    "    subset_texts = texts[start_idx:end_idx]\n",
    "    print(f\"Processing part {part + 1}/{num_parts} with {len(subset_texts)} texts.\")\n",
    "\n",
    "    # Encode the texts\n",
    "    start_time = time.time()\n",
    "    embeddings = model.encode(subset_texts, batch_size=256, device=device, show_progress_bar=True)\n",
    "    print(f\"Part {part + 1} encoded in {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "    # Build Annoy index for this part\n",
    "    t = AnnoyIndex(f, 'euclidean')\n",
    "    for i, embedding in enumerate(embeddings):\n",
    "        t.add_item(i, embedding)\n",
    "\n",
    "    print(f\"Building Annoy index for part {part + 1}...\")\n",
    "    t.build(10)  # Use 10 trees for better accuracy\n",
    "    index_file = os.path.join(output_dir, f\"annoy_index_part_{part + 1}.ann\")\n",
    "    t.save(index_file)\n",
    "    print(f\"Part {part + 1} Annoy index saved to {index_file}.\")\n",
    "\n",
    "print(\"All parts processed and Annoy indexes created successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
