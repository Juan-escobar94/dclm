{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74b5046d-56e6-4fa0-aeae-6ccb42da381a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!source dclm_env/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2241efaa-0a18-40e3-9ae1-f2396c22f2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from disk...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be53154f2d8d454ba7b61a463cbce2d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n",
      "Loading SentenceTransformer model...\n",
      "Model loaded successfully!\n",
      "Initialized Annoy index.\n",
      "Starting to process dataset in batches...\n",
      "Processing split: train\n",
      "Processed 100000 items so far.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m text \u001b[38;5;241m=\u001b[39m example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Extract the 'text' field directly from the example\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Encode the sentence\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Encode the single text\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Add embedding to Annoy index\u001b[39;00m\n\u001b[1;32m     34\u001b[0m t\u001b[38;5;241m.\u001b[39madd_item(i, embedding)\n",
      "File \u001b[0;32m/data/horse/ws/komo978d-datacomp/dclm_env/lib/python3.9/site-packages/sentence_transformers/SentenceTransformer.py:535\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    532\u001b[0m     ht\u001b[38;5;241m.\u001b[39mhpu\u001b[38;5;241m.\u001b[39mwrap_in_hpu_graph(\u001b[38;5;28mself\u001b[39m, disable_tensor_cache\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    533\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_hpu_graph_enabled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 535\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m show_progress_bar \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    537\u001b[0m     show_progress_bar \u001b[38;5;241m=\u001b[39m logger\u001b[38;5;241m.\u001b[39mgetEffectiveLevel() \u001b[38;5;129;01min\u001b[39;00m (logging\u001b[38;5;241m.\u001b[39mINFO, logging\u001b[38;5;241m.\u001b[39mDEBUG)\n",
      "File \u001b[0;32m/data/horse/ws/komo978d-datacomp/dclm_env/lib/python3.9/site-packages/torch/nn/modules/module.py:2449\u001b[0m, in \u001b[0;36mModule.eval\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21meval\u001b[39m(\u001b[38;5;28mself\u001b[39m: T) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m   2434\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Set the module in evaluation mode.\u001b[39;00m\n\u001b[1;32m   2435\u001b[0m \n\u001b[1;32m   2436\u001b[0m \u001b[38;5;124;03m    This has any effect only on certain modules. See documentations of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2447\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m   2448\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/horse/ws/komo978d-datacomp/dclm_env/lib/python3.9/site-packages/torch/nn/modules/module.py:2430\u001b[0m, in \u001b[0;36mModule.train\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m   2428\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m mode\n\u001b[1;32m   2429\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m-> 2430\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2431\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/data/horse/ws/komo978d-datacomp/dclm_env/lib/python3.9/site-packages/torch/nn/modules/module.py:2430\u001b[0m, in \u001b[0;36mModule.train\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m   2428\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m mode\n\u001b[1;32m   2429\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m-> 2430\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2431\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "    \u001b[0;31m[... skipping similar frames: Module.train at line 2430 (3 times)]\u001b[0m\n",
      "File \u001b[0;32m/data/horse/ws/komo978d-datacomp/dclm_env/lib/python3.9/site-packages/torch/nn/modules/module.py:2430\u001b[0m, in \u001b[0;36mModule.train\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m   2428\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m mode\n\u001b[1;32m   2429\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m-> 2430\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2431\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/data/horse/ws/komo978d-datacomp/dclm_env/lib/python3.9/site-packages/torch/nn/modules/module.py:2429\u001b[0m, in \u001b[0;36mModule.train\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m   2427\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining mode is expected to be boolean\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2428\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m mode\n\u001b[0;32m-> 2429\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m   2430\u001b[0m     module\u001b[38;5;241m.\u001b[39mtrain(mode)\n\u001b[1;32m   2431\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/data/horse/ws/komo978d-datacomp/dclm_env/lib/python3.9/site-packages/torch/nn/modules/module.py:2319\u001b[0m, in \u001b[0;36mModule.children\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2313\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mchildren\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModule\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m   2314\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Return an iterator over immediate children modules.\u001b[39;00m\n\u001b[1;32m   2315\u001b[0m \n\u001b[1;32m   2316\u001b[0m \u001b[38;5;124;03m    Yields:\u001b[39;00m\n\u001b[1;32m   2317\u001b[0m \u001b[38;5;124;03m        Module: a child module\u001b[39;00m\n\u001b[1;32m   2318\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2319\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnamed_children():\n\u001b[1;32m   2320\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m module\n",
      "File \u001b[0;32m/data/horse/ws/komo978d-datacomp/dclm_env/lib/python3.9/site-packages/torch/nn/modules/module.py:2337\u001b[0m, in \u001b[0;36mModule.named_children\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2323\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.\u001b[39;00m\n\u001b[1;32m   2324\u001b[0m \n\u001b[1;32m   2325\u001b[0m \u001b[38;5;124;03mYields:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2334\u001b[0m \n\u001b[1;32m   2335\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2336\u001b[0m memo \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m-> 2337\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_modules\u001b[49m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   2338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m module \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m memo:\n\u001b[1;32m   2339\u001b[0m         memo\u001b[38;5;241m.\u001b[39madd(module)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from annoy import AnnoyIndex\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_from_disk\n",
    "\n",
    "\n",
    "\n",
    "# Load the dataset from the relative directory\n",
    "print(\"Loading dataset from disk...\")\n",
    "ds = load_from_disk('./wikipedia20231101en')\n",
    "print(\"Dataset loaded successfully!\")\n",
    "\n",
    "# Load the SentenceTransformer model\n",
    "print(\"Loading SentenceTransformer model...\")\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Initialize Annoy index\n",
    "f = model.get_sentence_embedding_dimension()\n",
    "t = AnnoyIndex(f, 'euclidean')\n",
    "print(\"Initialized Annoy index.\")\n",
    "\n",
    "# Process dataset in batches\n",
    "i = 0\n",
    "print(\"Starting to process dataset in batches...\")\n",
    "for split in ds.keys():\n",
    "    print(f\"Processing split: {split}\")\n",
    "    for example in ds[split]:\n",
    "        text = example['text']  # Extract the 'text' field directly from the example\n",
    "        \n",
    "        # Encode the sentence\n",
    "        embedding = model.encode(text)  # Encode the single text\n",
    "        \n",
    "        # Add embedding to Annoy index\n",
    "        t.add_item(i, embedding)\n",
    "        i += 1\n",
    "\n",
    "        # Print progress every 10000 items\n",
    "        if i % 100000 == 0:\n",
    "            print(f\"Processed {i} items so far.\")\n",
    "\n",
    "\n",
    "print(\"Finished processing all batches!\")\n",
    "\n",
    "# Build and save the Annoy index\n",
    "print(\"Building Annoy index...\")\n",
    "t.build(10)  # Use more trees for better accuracy\n",
    "print(\"Annoy index built successfully!\")\n",
    "\n",
    "print(\"Saving Annoy index...\")\n",
    "t.save('highQualityAnnoyEuclidean.ann')\n",
    "print(\"Annoy index saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaf4e61-837f-4140-bacc-f2aa83031a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from disk...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1ba6ada3d044d90984fd05a59a4a54a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n",
      "Loading SentenceTransformer model...\n",
      "Model loaded successfully!\n",
      "Initialized Annoy index.\n"
     ]
    }
   ],
   "source": [
    "from annoy import AnnoyIndex\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_from_disk\n",
    "from multiprocessing import Pool\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "print(\"Loading dataset from disk...\")\n",
    "ds = load_from_disk('./wikipedia20231101en')\n",
    "print(\"Dataset loaded successfully!\")\n",
    "\n",
    "# Load the SentenceTransformer model\n",
    "print(\"Loading SentenceTransformer model...\")\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Initialize Annoy index\n",
    "f = model.get_sentence_embedding_dimension()\n",
    "t = AnnoyIndex(f, 'euclidean')\n",
    "print(\"Initialized Annoy index.\")\n",
    "\n",
    "def process_batch(batch):\n",
    "    \"\"\"\n",
    "    Function to process a batch of texts.\n",
    "    Encodes texts and returns a list of tuples (index, embedding).\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for idx, example in batch:\n",
    "        text = example['text']\n",
    "        embedding = model.encode(text)  # Encode the single text\n",
    "        results.append((idx, embedding))\n",
    "    return results\n",
    "\n",
    "# Prepare dataset for parallel processing\n",
    "all_data = [(i, example) for split in ds.keys() for i, example in enumerate(ds[split])]\n",
    "\n",
    "# Split data into batches for parallel processing\n",
    "batch_size = 1000  # Adjust batch size based on available memory\n",
    "batches = [all_data[i:i + batch_size] for i in range(0, len(all_data), batch_size)]\n",
    "\n",
    "print(\"Starting parallel processing...\")\n",
    "with Pool() as pool:\n",
    "    results = pool.map(process_batch, batches)\n",
    "\n",
    "# Add items to Annoy index\n",
    "print(\"Adding items to Annoy index...\")\n",
    "for batch_results in results:\n",
    "    for idx, embedding in batch_results:\n",
    "        t.add_item(idx, embedding)\n",
    "\n",
    "# Build and save the Annoy index\n",
    "print(\"Building Annoy index...\")\n",
    "t.build(10)  # Use more trees for better accuracy\n",
    "print(\"Annoy index built successfully!\")\n",
    "\n",
    "print(\"Saving Annoy index...\")\n",
    "t.save('highQualityAnnoyEuclidean.ann')\n",
    "print(\"Annoy index saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b865412-558c-42b9-95cf-4fb51feb2b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\konrad_master\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from disk...\n",
      "Dataset loaded successfully!\n",
      "Loading SentenceTransformer model...\n",
      "Model loaded successfully!\n",
      "Initialized Annoy index.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:   0%|          | 0/6408 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting parallel processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\konrad_master\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "from annoy import AnnoyIndex\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "# Load the dataset\n",
    "# Load the dataset from the relative directory\n",
    "print(\"Loading dataset from disk...\")\n",
    "\n",
    "ds = load_dataset(\"wikimedia/wikipedia\", \"20231101.en\")\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "\n",
    "# Load the SentenceTransformer model\n",
    "print(\"Loading SentenceTransformer model...\")\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Initialize Annoy index\n",
    "f = model.get_sentence_embedding_dimension()\n",
    "t = AnnoyIndex(f, 'euclidean')\n",
    "print(\"Initialized Annoy index.\")\n",
    "\n",
    "def process_batch(batch):\n",
    "    \"\"\"\n",
    "    Function to process a batch of texts.\n",
    "    Encodes texts and returns a list of tuples (index, embedding).\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for idx, example in batch:\n",
    "        text = example['text']\n",
    "        embedding = model.encode(text)  # Encode the single text\n",
    "        results.append((idx, embedding))\n",
    "    return results\n",
    "\n",
    "# Prepare dataset for parallel processing\n",
    "all_data = [(i, example) for split in ds.keys() for i, example in enumerate(ds[split])]\n",
    "\n",
    "# Split data into batches for parallel processing\n",
    "batch_size = 1000  # Adjust batch size based on available memory\n",
    "batches = [all_data[i:i + batch_size] for i in range(0, len(all_data), batch_size)]\n",
    "\n",
    "# Create a progress bar\n",
    "progress_bar = tqdm(total=len(batches), desc=\"Processing Batches\")\n",
    "\n",
    "def update_progress(*_):\n",
    "    \"\"\"\n",
    "    Callback to update the progress bar.\n",
    "    \"\"\"\n",
    "    progress_bar.update()\n",
    "\n",
    "# Use Pool for parallel processing\n",
    "print(\"Starting parallel processing...\")\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "with ThreadPool() as pool:\n",
    "    results = list(tqdm(pool.imap(process_batch, batches), total=len(batches), desc=\"Processing Batches\"))\n",
    "\n",
    "# Close the progress bar\n",
    "progress_bar.close()\n",
    "\n",
    "# Add items to Annoy index\n",
    "print(\"Adding items to Annoy index...\")\n",
    "for batch_results in results:\n",
    "    for idx, embedding in batch_results:\n",
    "        t.add_item(idx, embedding)\n",
    "\n",
    "# Build and save the Annoy index\n",
    "print(\"Building Annoy index...\")\n",
    "t.build(10)  # Use more trees for better accuracy\n",
    "print(\"Annoy index built successfully!\")\n",
    "\n",
    "print(\"Saving Annoy index...\")\n",
    "t.save('highQualityAnnoyEuclidean.ann')\n",
    "print(\"Annoy index saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5f09422-de35-49fc-9fe9-9a7aa682f67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from disk...\n",
      "Dataset loaded successfully!\n",
      "Loading SentenceTransformer model...\n",
      "Model loaded successfully!\n",
      "{'train': 6407814}\n"
     ]
    }
   ],
   "source": [
    "from annoy import AnnoyIndex\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "\n",
    "# Load the dataset from the relative directory\n",
    "print(\"Loading dataset from disk...\")\n",
    "ds = load_dataset(\"wikimedia/wikipedia\", \"20231101.en\")\n",
    "print(\"Dataset loaded successfully!\")\n",
    "\n",
    "# Load the SentenceTransformer model\n",
    "print(\"Loading SentenceTransformer model...\")\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "print(ds.num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b811a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Annoy index.\n",
      "Starting to process dataset in batches...\n"
     ]
    }
   ],
   "source": [
    "# Initialize Annoy index\n",
    "f = model.get_sentence_embedding_dimension()\n",
    "t = AnnoyIndex(f, 'euclidean')\n",
    "print(\"Initialized Annoy index.\")\n",
    "\n",
    "# Process dataset in batches\n",
    "i = 0\n",
    "print(\"Starting to process dataset in batches...\")\n",
    "sentences = model.encode(ds['train'])\n",
    "for embedding in sentences:\n",
    "    t.add_item(i, embedding)\n",
    "    i += 1\n",
    "    if i % 100000 == 0:\n",
    "        print(f\"Processed {i} items so far.\")\n",
    "\n",
    "\n",
    "print(\"Finished processing all batches!\")\n",
    "\n",
    "# Build and save the Annoy index\n",
    "print(\"Building Annoy index...\")\n",
    "t.build(10)  # Use more trees for better accuracy\n",
    "print(\"Annoy index built successfully!\")\n",
    "\n",
    "print(\"Saving Annoy index...\")\n",
    "t.save('highQualityAnnoyEuclidean.ann')\n",
    "print(\"Annoy index saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
