{
    "name": "dclm_refined_tud_tokenized",
    "sources": "",
    "tokenized": true,
    "num_tokens": 2049,
    "size": null,
    "dataset_url": "/data/horse/ws/jori152b-datacomp_data/dclm_refined_tud/tokenized-shf-output/",
    "manifest_url": "/data/horse/ws/jori152b-datacomp_data/dclm_refined_tud/tokenized-shf-output/manifest.jsonl",
    "dcnlp_commit_hash": "",
    "dcnlp_diff": "",
    "uuid": "a1675e2c-db3a-47c9-8a2a-52c3e2ecadae",
    "tokenizer": "EleutherAI/gpt-neox-20b",
    "sampling_yaml": null
}
